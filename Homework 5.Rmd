---
title: "Homework 5"
author: "Vinny Sriram, Lee-Ann Kao, Jennifer Centa"
date: "2024-03-31"
output: word_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

**We are working with the a Data Set that tracks the democraticness, economic conditions, and foreign aid, of post colonial countries across time. The group members are Vinny Sriram, Lee-Ann Kao, and Jennifer Centa. Our emails are vinny.sriram@yale.edu, lee-ann.kao@yale.edu, and jennifer.centa@yale.edu.**

```{r}
par(ask=FALSE)
#load relevant libraries
library(aplpack)
library(fpc)
library(cluster)
library(ape)
library(amap)
library(dplyr)
```

### Set-Up
```{r}
data <- read.csv("data.csv")
data <- subset(data, year == 2004)
data <- data[!duplicated(data[c('country')]), ]
```


### Question 1: Distance metrics

*We are going to perform cluster analysis looking at 83 countries based on the characteristics of education, log exports, FDI score, log imports, and log GDP in 2004 (83 objects and 5 variables). All of these are continuous variables, and we are clustering based on our cases/objects. Because of this, distance metrics such as Euclidean distance and Manhattan distance (examples of Minkowski distance), as well as maximum distance, Squared Euclidean distance, and Sorenson distance, are most appropriate for our data.*

*However, all of these measures are scale variant, meaning that we want to subtract the sample means and divide by the sample standard deviation to avoid some variables having more influence than others (or use Relative Euclidean distance). For our data there is no case in which we want more inherent variability to have more of an influence on the overall calculation of distance, and therefore we chose to standardize our data.*

```{r}
#standardize
datanorm <- data[, c("educm","logexp", "fdi", "logimp", "loggdpc")]
rownames(datanorm) <- data[, 1]
datanorm <- scale(na.omit(datanorm))
dim(datanorm)
head(datanorm)
```



### Question 2: Hierarchical cluster analysis

```{r}
#Techniques A: Euclidean distance + complete linkage

#distance matrix using Euclidean distance
dist1 <- dist(datanorm, method = "euclidean")

#clustering using complete linkage
clust1 <- hclust(dist1, method = "complete")

#dendrogram
plot(clust1, labels = rownames(datanorm), cex = 0.4, xlab = "", ylab = "Distance (Euclidean)", main = "Clustering of Countries (Complete Linkage)")


#Techniques B: Manhattan distance + Ward's method

#distance matrix using Manhattan distance
dist2 <- dist(datanorm, method = "manhattan")

#clustering using Ward's method
clust2 <- hclust(dist2, method = "ward.D")

#dendrogram
plot(clust2, labels = rownames(datanorm), cex = 0.4, xlab = "", ylab = "Distance (Manhattan)", main = "Clustering of Countries (Ward's Method)")


#Techniques C: Maximum distance + single linkage

#distance matrix using maximum distance
dist3 <- dist(datanorm, method = "maximum")

#clustering using single linkage
clust3 <- hclust(dist3, method = "single")

#dendrogram
plot(clust3, labels = rownames(datanorm), cex = 0.4, xlab = "", ylab = "Distance (Maximum)", main = "Clustering of Countries (Single Linkage)")
```
*With Euclidean distance and Complete linkage we see a "skewed right" tree, meaning that clusters on the right tend to be larger than the cluster to the left of them, and there also appears to be an "outlier" with Chad having its own branch at the top along with the two main clusters. With Manhattan Distance and Ward's Method, the tree is more symmetrical with larger distances between them than before, and Chad is now incorporated into the clusters to create two main clusters. The similar results but increased distance makes sense because Ward's method tends to produce similar results to complete linkage, but as you move up the distances increase quadratically rather than linearly, pushing individuals into discrete groups more rapidly. Single linkage is extremely "skewed right" or asymmetrical, producing not the best cluster results as discussed in lecture.*



### Question 3: R function

```{r}
source("http://reuningscherer.net/multivariate/R/HClusEval3.R.txt")
#Call the function
hclus_eval(datanorm, dist_m = 'euclidean', clus_m = 'complete', plot_op = T, print_num = 15)
```
*By looking at this graph, we can estimate that the optimal group number is 4. The elbows of each line (RSQ, RMSSTD, and SPRSQ) are roughly at 4, with big increases/decreases up until that point, then moderate change afterwards. The plot of cluster distance also supports this observation, with the main elbow of the graph at 4.*



### Question 4: k-means clustering
*Run k-means clustering on your data. Compare results to what you got in 3.) Include a sum of squares vs. k (number of clusters) plot and comment on how many groups exist.*

```{r}
kdata <- datanorm
n.lev <- 15 

# Calculate the within groups sum of squared error (SSE) for the number of cluster solutions selected by the user
wss <- rnorm(10)
while (prod(wss==sort(wss,decreasing=T))==0) {
  wss <- (nrow(kdata)-1)*sum(apply(kdata,2,var))
  for (i in 2:n.lev) wss[i] <- sum(kmeans(kdata, centers=i)$withinss)}

# Calculate the within groups SSE for 250 randomized data sets (based on the original input data)
k.rand <- function(x){
  km.rand <- matrix(sample(x),dim(x)[1],dim(x)[2])
  rand.wss <- as.matrix(dim(x)[1]-1)*sum(apply(km.rand,2,var))
  for (i in 2:n.lev) rand.wss[i] <- sum(kmeans(km.rand, centers=i)$withinss)
  rand.wss <- as.matrix(rand.wss)
  return(rand.wss)
}

rand.mat <- matrix(0,n.lev,250)

k.1 <- function(x) { 
  for (i in 1:250) {
    r.mat <- as.matrix(suppressWarnings(k.rand(kdata)))
    rand.mat[,i] <- r.mat}
  return(rand.mat)
}

# Same function as above for data with < 3 column variables
k.2.rand <- function(x){
  rand.mat <- matrix(0,n.lev,250)
  km.rand <- matrix(sample(x),dim(x)[1],dim(x)[2])
  rand.wss <- as.matrix(dim(x)[1]-1)*sum(apply(km.rand,2,var))
  for (i in 2:n.lev) rand.wss[i] <- sum(kmeans(km.rand, centers=i)$withinss)
  rand.wss <- as.matrix(rand.wss)
  return(rand.wss)
}

k.2 <- function(x){
  for (i in 1:250) {
    r.1 <- k.2.rand(kdata)
    rand.mat[,i] <- r.1}
  return(rand.mat)
}

# Determine if the data data table has > or < 3 variables and call appropriate function above
if (dim(kdata)[2] == 2) { rand.mat <- k.2(kdata) } else { rand.mat <- k.1(kdata) }

# Plot within groups SSE against all tested cluster solutions for actual and randomized data - 1st: Log scale, 2nd: Normal scale

xrange <- range(1:n.lev)
yrange <- range(log(rand.mat),log(wss))
plot(xrange,yrange, type='n', xlab='Cluster Solution', ylab='Log of Within Group SSE', main='Cluster Solutions against Log of SSE')
for (i in 1:250) lines(log(rand.mat[,i]),type='l',col='red')
lines(log(wss), type="b", col='blue')
legend('topright',c('Actual Data', '250 Random Runs'), col=c('blue', 'red'), lty=1)

yrange <- range(rand.mat,wss)
plot(xrange,yrange, type='n', xlab="Cluster Solution", ylab="Within Groups SSE", main="Cluster Solutions against SSE")
for (i in 1:250) lines(rand.mat[,i],type='l',col='red')
lines(1:n.lev, wss, type="b", col='blue')
legend('topright',c('Actual Data', '250 Random Runs'), col=c('blue', 'red'), lty=1)

# Calculate the mean and standard deviation of difference between SSE of actual data and SSE of 250 randomized datasets
r.sse <- matrix(0,dim(rand.mat)[1],dim(rand.mat)[2])
wss.1 <- as.matrix(wss)
for (i in 1:dim(r.sse)[2]) {
  r.temp <- abs(rand.mat[,i]-wss.1[,1])
  r.sse[,i] <- r.temp}
r.sse.m <- apply(r.sse,1,mean)
r.sse.sd <- apply(r.sse,1,sd)
r.sse.plus <- r.sse.m + r.sse.sd
r.sse.min <- r.sse.m - r.sse.sd

# Plot difference between actual SSE mean SSE from 250 randomized datasets - 1st: Log scale, 2nd: Normal scale 

xrange <- range(1:n.lev)
if (min(r.sse.min) < 0){
   yrange <- range(log(r.sse.plus - min(r.sse.min)*1.05), log(r.sse.min - min(r.sse.min)*1.05))
} else {
   yrange <- range(log(r.sse.plus), log(r.sse.min))
}

plot(xrange,yrange, type='n',xlab='Cluster Solution', ylab='Log of SSE - Random SSE', main='Cluster Solustions against (Log of SSE - Random SSE)')
lines(log(r.sse.m), type="b", col='blue')
lines(log(r.sse.plus), type='l', col='red')
lines(log(r.sse.min), type='l', col='red')
legend('topright',c('SSE - random SSE', 'SD of SSE-random SSE'), col=c('blue', 'red'), lty=1)

xrange <- range(1:n.lev)
yrange <- range(r.sse.plus,r.sse.min)
plot(xrange,yrange, type='n',xlab='Cluster Solution', ylab='SSE - Random SSE', main='Cluster Solutions against (SSE - Random SSE)')
lines(r.sse.m, type="b", col='blue')
lines(r.sse.plus, type='l', col='red')
lines(r.sse.min, type='l', col='red')
legend('topright',c('SSE - random SSE', 'SD of SSE-random SSE'), col = c('blue', 'red'), lty = 1)

# Select the appropriate number of clusters- 5 because of SSE elbow
clust.level <- 5

# Apply K-means cluster solutions - append clusters to CSV file
fit <- kmeans(kdata, clust.level)
aggregate(kdata, by=list(fit$cluster), FUN=mean)
clust.out <- fit$cluster
kclust <- as.matrix(clust.out)
kclust.out <- cbind(kclust, datanorm)
write.table(kclust.out, file="kmeans_out.csv", sep=",")


```
*The results are not too far off of question 3, as the cluster solutions against SSE plot shows that there is an elbow around 5, with more than 5 clusters not helping us much. For this reason, 5 clusters was selected to apply K-means cluster solutions, which is shown in the five cluster solution in DA space. That plot shows clear groupings, with a few overlaps.*

### Question 5
```{r}
# Display Principal Components plot of data with clusters identified

clusplot(kdata, fit$cluster, shade = F, labels = 2, lines = 0, color = T, lty = 4, main = 'Principal Components plot showing K-means clusters')

plotcluster(kdata, fit$cluster, main="Five Cluster Solution in DA Space",
            xlab="First Discriminant Function", ylab="Second Discriminant Function")

for (i in 1:5){
  print(paste("Countries in Cluster ",i))
  print(rownames(datanorm)[fit$cluster == i])
  print (" ")
}

```

### Question 6
*Based on the plots of the result sin the discriminant analysis space and PCA space there appear to be 5 groups. One group consists of just one country: Chad, which is an extreme outlier.The remaining 4 groups are more concentrated in an elliptical distribution. There is one group on the left, one on the right, one on the center top, and one on the center bottom of the elliptical distribution.*

### Question 7
*Besides the first group which contains the outlier Chad (which makes sense given Chad experienced a coup attemp), the other four groups do not seem to make sense on the surface. One group contains 13 countries, another 19, another 20, and another 27, but there are no discernible categorical characteristics that can be discerned between these groups. The original data set contains some categorical variables such as region and regime type, but neither of these align with the groups found through cluster analysis. However, given the included variables for the cluster analysis all describe economic conditions of the country it is likely that the groupings demonstrate similar economic conditions in 2004 in these countries. Further data would be needed to make full sense/explain the categories that these groups represent.*