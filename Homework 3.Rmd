---
title: "Homework 2"
author: "Vinny Sriram, Lee-Ann Kao, Jennifer Centa"
date: "2024-02-03"
output: word_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

**We are working with the Pizza Data Set. The group members are Vinny Sriram, Lee-Ann Kao, and Jennifer Centa. Our emails are vinny.sriram@yale.edu, lee-ann.kao@yale.edu, and jennifer.centa@yale.edu.**

```{r, echo=FALSE}
library(MASS)
library(biotools)
library(klaR)
library(car)
library(dplyr)
library(lubridate)
library(ggplot2)
library(ggExtra)
library(heplots)
```

# Set-Up
```{r}
data <- read.csv("data.csv")
data <- subset(data, year == 2004)

data <- subset(data, regime %in% c(2, 3, 4))
table(data$regime)
```


### Question 1

```{r}
# Check multivariate normality within each group
source("http://www.reuningscherer.net/multivariate/R/CSQPlot.r.txt")
par(pty = "s", cex = 0.8, cex.main = 0.8)
CSQPlot(data[data$regime == 2, c(6:10)], label = "Presidential Democracy")
CSQPlot(data[data$regime == 3, c(6:10)], label = "Civilian Dictatorship")
CSQPlot(data[data$regime == 4, c(6:10)], label = "Military Dictatorship")

# Covariance Compairson
boxM(data[,c(6:10)], data$regime)

#Matrix Plots
plot(data[,6:10], col = data[,12]+2, pch = data[,12] + 15, cex = 1.2)

data <- data[data$country != "Chad", ]
data <- data[data$country != "Singapore", ]

# Covariance Compairson
boxM(data[,c(6:10)], data$regime)

#Matrix Plots
plot(data[,6:10], col = data[,12]+2, pch = data[,12] + 15, cex = 1.2)
```

*Based on the Chi-Square Quantile Plots, each group has multivariate normal data because all the data points remain within the 95% confidence bounds. However, there is not similar covariance across groups given the p-value from the Box's M test of 1.879e-09, forcing us to reject the null-hypothesis of homogeneous covariance Looking at the matrix plots to look at the two variable relationships, for the most part the correlations across two variables seem similar across the three groups. However, there is an extreme outlier for fdi (Chad), and a significant outlier in both log imports and log GDP per capita (Singapore). It seems reasonable to remove Chad, especially when considering an outlying event (failed coup) that may have caused this extreme fdi in this year. Furthermore, we can justify removing Singapore as in 2004 it fits outside of the developing-post colonial group that all other countries belong to. Once removing these two entries, we can see that the covariance across groups has gotten closer, the Box's M-test p-value now being 0.0015 > 0.001, so we can fail to reject the null hypotehsis and move forward with linear DA. Since the Box's M-test is very sensitive in larger samples, we can use a lower alpha of 0.001.*

### Question 2

```{r}
#Linear Discriminant Analysis with equal priors
(data.disc <- lda(data[, 6:10], grouping = data$regime, priors = c(.333, .333, .333)))

#Step-Wise
step1 <- stepclass(regime ~ educm + logexp + fdi + logimp + loggdpc, data = data, method = "lda", direction = "both", priors = c(.333, .333, .333), fold = nrow(data))
```
*Linear Discriminant Analysis seems like the best model because the Box's M-Test indicated similar covariance across the groups. Performing step-wise dicriminant analysis indicated the sole variable loggdpc can predict group, but the classification accuracy is 0.49, lower than the regular non cross-validadated linear discriminant analysis. *


### Question 3
```{r}
data.manova <- manova(as.matrix(data[, 6:10]) ~ data$regime)
summary.manova(data.manova, test = "Wilks")
```
*The p-value on our Wilk's Lambda is 0.01961 < alpha at 0.05, so we reject the null hypothesis that the multivariate means are the same. In other words, that is statistical evidence that the multivariate group means are different among regimes.*


### Question 4

*We have 2 significant discriminate functions (3 regime types - 1 = 2, which is the minimum compared to our number of variables). Because the eigenvectors are ordered according to discriminating ability, the first function has the greatest relative discriminating power. This can also be seen by the proportion of trace, where our first function has a score of 0.8621 compared to 0.1379 for the second function. These scores are the between-class variance that is explained by the discriminant functions or in other words, their relative discriminating power.*


### Question 5
```{r}
#Classification Results
(ctraw <- table(data$regime, predict(data.disc)$class))
round(sum(diag(prop.table(ctraw))), 2)

data.discCV <- lda(data[,6:10], grouping = data$regime, CV = TRUE)
(ctCV <- table(data$regime, data.discCV$class))
round(sum(diag(prop.table(ctCV))), 2)
```
*Upon performing the linear discriminant analysis we can see the classification accuracy is 0.54 for non cross-validated data and 0.46 for cross-validated data. Given there are three groups with relatively equal frequency, 0.54 is not a terrible classification rate for discriminating ability. There is evidence for over-fitting in this case because as soon as we cross-validate our results, our overall percentage correct drops by 8%.*


### Question 6
```{r}
print("Standardized Coefficients")
round(lda(scale(data[, 6:10]), grouping = data$regime, priors = c(1/3, 1/3, 1/3))$scaling, 2)
```
*All of the standardized coefficients have relatively similar magnitudes (around 0.70), suggesting relatively equal importance of variables. However, I would exclude fdi from the list, as its coefficients are smaller in magnitude than the rest (around 0.30). Therefore, I would consider education, log exports, log imports, and log GDP to be the "best" discriminators among groups. However, it should be noted that log exports varies significantly in its coefficient magnitude between functions. This is likely because log exports and log imports are highly correlated (based on the matrix plots), and are therefore competing to discriminate between groups. The correlation between variables means that caution must be taken when making inferences about relative discriminating power.*
